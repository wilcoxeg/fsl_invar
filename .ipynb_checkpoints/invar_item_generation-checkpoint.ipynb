{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from random import sample \n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>WORD</th>\n",
       "      <th>LEMMA</th>\n",
       "      <th>UPOS</th>\n",
       "      <th>XPOS</th>\n",
       "      <th>FEATS</th>\n",
       "      <th>HEAD</th>\n",
       "      <th>DEPREL</th>\n",
       "      <th>DEPS</th>\n",
       "      <th>MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pierre</td>\n",
       "      <td>Pierre</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>_</td>\n",
       "      <td>2</td>\n",
       "      <td>compound</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vinken</td>\n",
       "      <td>Vinken</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>_</td>\n",
       "      <td>9</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>_</td>\n",
       "      <td>2</td>\n",
       "      <td>punct</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>_</td>\n",
       "      <td>5</td>\n",
       "      <td>nummod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>years</td>\n",
       "      <td>year</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>_</td>\n",
       "      <td>6</td>\n",
       "      <td>nmod:npmod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    WORD   LEMMA   UPOS XPOS FEATS  HEAD      DEPREL DEPS MISC\n",
       "0      1  Pierre  Pierre  PROPN  NNP     _     2    compound    _    _\n",
       "1      2  Vinken  Vinken  PROPN  NNP     _     9       nsubj    _    _\n",
       "2      3       ,       ,  PUNCT    ,     _     2       punct    _    _\n",
       "3      4      61      61    NUM   CD     _     5      nummod    _    _\n",
       "4      5   years    year   NOUN  NNS     _     6  nmod:npmod    _    _"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "data = pd.read_csv(\"data/wsj_deps.conllu\", sep=\"\\t\", header=None)\n",
    "data.columns = [\"index\", \"WORD\", \"LEMMA\", \"UPOS\", \"XPOS\", \"FEATS\", \"HEAD\", \"DEPREL\", \"DEPS\", \"MISC\"]\n",
    "train_items = set([x[0] for x in flatten(pickle.load(open(\"data/train_pos.p\", \"rb\")))])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times each token occurs as per its part of speech\n",
    "verbs = data[(data[\"XPOS\"] == \"VBD\") | (data[\"XPOS\"] == \"VBN\") | (data[\"XPOS\"] == \"VB\")]\n",
    "verbs = verbs[verbs[\"WORD\"].str.islower()]\n",
    "verbs = verbs[verbs[\"WORD\"].isin(train_items)]\n",
    "\n",
    "v_counts = verbs.groupby([\"WORD\", \"LEMMA\", \"XPOS\"]).size().reset_index()\n",
    "#v_counts.to_csv(\"data/v_counts.csv\")\n",
    "\n",
    "# Just some code to print out common words from the training (useful for deciding what to put in the test sentences)\n",
    "advs = data[(data[\"XPOS\"] == \"VB\")]\n",
    "adv_counts = advs.groupby([\"WORD\", \"LEMMA\", \"XPOS\"]).size().reset_index()\n",
    "adv_counts = adv_counts.sort_values(by=[0], ascending=False)\n",
    "adv_counts = [tuple(r) for r in adv_counts.values.tolist()]\n",
    "#print(adv_counts[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITEMS_PER_TEST = 20\n",
    "MAX_ITEMS_PER_BUCKET = 20\n",
    "\n",
    "# Open class items\n",
    "NOUN_BLOCKS = [\"investor\", \"maker\", \"president\", \"executive\", \"officer\", \"lawyer\", \"government\", \"chairman\", \"spokesman\"]\n",
    "V_TR_BLOCKS = [\"likes\", \"saw\", \"had\"]\n",
    "ADJ_BLOCKS = [\"big\", \"important\", \"new\", \"special\", \"key\", \"foreign\", \"old\"]\n",
    "ITEM_BLOCKS_SING = [\"report\", \"investment\", \"position\", \"letter\", \"idea\", \"thing\", \"computer\", \"product\", \"paper\"]\n",
    "ITEM_BLOCKS_PL = [\"reports\", \"investments\", \"position\", \"letters\", \"ideas\", \"things\", \"computers\", \"products\", \"papers\"]\n",
    "PRES_ADVS = [\"quickly\", \"directly\", \"immediately\", \"easily\", \"rapidly\", \"entirely\"]\n",
    "\n",
    "# Closed class items\n",
    "DET_BLOCKS = [\"the\", \"some\", \"the\", \"the\", \"the\", \"the\"]\n",
    "INTENSE_BLOCKS = [\"very\", \"extremely\", \"somewhat\", \"rather\"]\n",
    "PAST_ADVS = [\"yesterday\", \"recently\", \"today\"]\n",
    "AUX = [\"can\", \"will\", \"might\", \"should\", \"can\", \"will\", \"can\", \"will\"]\n",
    "PREPS = [\"near\", \"with\"]\n",
    "SING_BE = [\"is\", \"was\"]\n",
    "PL_BE = [\"are\", \"were\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_words(data):\n",
    "    result = []\n",
    "    \n",
    "    \"\"\"\n",
    "    tiny_exp = data[(data[0] > 1) & (data[0] <= 2)]\n",
    "    result.extend(sample([tuple(r) for r in tiny_exp.values.tolist()], min(MAX_ITEMS_PER_BUCKET, tiny_exp.shape[0])))\n",
    "    \n",
    "    small_exp = data[(data[0] > 5) & (data[0] <= 10)]\n",
    "    result.extend(sample([tuple(r) for r in small_exp.values.tolist()], min(MAX_ITEMS_PER_BUCKET, small_exp.shape[0])))\n",
    "    \n",
    "    mid_exp = data[(data[0] > 10) & (data[0] <= 20)]\n",
    "    result.extend(sample([tuple(r) for r in mid_exp.values.tolist()], min(MAX_ITEMS_PER_BUCKET, mid_exp.shape[0])))\n",
    "\n",
    "    big_exp = data[(data[0] > 20) & (data[0] <= 50)]\n",
    "    result.extend(sample([tuple(r) for r in big_exp.values.tolist()], min(MAX_ITEMS_PER_BUCKET, big_exp.shape[0])))\n",
    "\n",
    "    huge_exp = data[(data[0] > 50)]\n",
    "    result.extend(sample([tuple(r) for r in huge_exp.values.tolist()], min(MAX_ITEMS_PER_BUCKET, huge_exp.shape[0])))\n",
    "    \"\"\"\n",
    "    \n",
    "    exp_2 = data[(data[0] > 1) & (data[0] <= 2)]\n",
    "    result.extend(sample([tuple(r) for r in exp_2.values.tolist()], min(MAX_ITEMS_PER_BUCKET, exp_2.shape[0])))\n",
    "            \n",
    "    exp_3 = data[(data[0] > 2) & (data[0] <= 3)]\n",
    "    result.extend(sample([tuple(r) for r in exp_3.values.tolist()], min(MAX_ITEMS_PER_BUCKET, exp_3.shape[0])))\n",
    "    \n",
    "    exp_4 = data[(data[0] > 3) & (data[0] <= 4)]\n",
    "    result.extend(sample([tuple(r) for r in exp_4.values.tolist()], min(MAX_ITEMS_PER_BUCKET, exp_4.shape[0])))\n",
    "     \n",
    "    exp_5 = data[(data[0] > 4) & (data[0] <= 5)]\n",
    "    result.extend(sample([tuple(r) for r in exp_5.values.tolist()], min(MAX_ITEMS_PER_BUCKET, exp_5.shape[0])))\n",
    "    \n",
    "    exp_10 = data[(data[0] > 5) & (data[0] <= 10)]\n",
    "    result.extend(sample([tuple(r) for r in exp_10.values.tolist()], min(MAX_ITEMS_PER_BUCKET, exp_10.shape[0])))\n",
    "    \n",
    "    exp_20 = data[(data[0] > 10) & (data[0] <= 20)]\n",
    "    result.extend(sample([tuple(r) for r in exp_20.values.tolist()], min(MAX_ITEMS_PER_BUCKET, exp_20.shape[0])))\n",
    "    \n",
    "    exp_50 = data[(data[0] > 20) & (data[0] <= 50)]\n",
    "    result.extend(sample([tuple(r) for r in exp_50.values.tolist()], min(MAX_ITEMS_PER_BUCKET, exp_50.shape[0])))\n",
    "    \n",
    "    exp_100 = data[(data[0] > 50) & (data[0] <= 100)]\n",
    "    result.extend(sample([tuple(r) for r in exp_100.values.tolist()], min(MAX_ITEMS_PER_BUCKET, exp_100.shape[0])))\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def export_plural_test(test, name):\n",
    "    test = flatten(test)\n",
    "    \n",
    "    items_output = []\n",
    "    for sent in test:\n",
    "        for word in sent[3].split(\" \"):\n",
    "            items_output.append((word, sent[0], sent[1], sent[2], sent[4], sent[5], sent[6], sent[7]))\n",
    "    \n",
    "    df = pd.DataFrame(items_output)\n",
    "    df.to_csv(name +\"_items.csv\", index=False, header=False)\n",
    "    \n",
    "    with open(name+\"_exposure_tests.txt\", \"w\") as outf:\n",
    "        outf.writelines(\"\\n\".join([x[3] for x in test]))\n",
    "\n",
    "\n",
    "def export_argstruct_test(test, name):\n",
    "    test = flatten(test)\n",
    "    \n",
    "    items_output = []\n",
    "    for sent in test:\n",
    "        for word in sent[5].split(\" \"):\n",
    "            items_output.append((word, sent[0], sent[1], sent[2], sent[3], sent[4], sent[6], sent[7], sent[8], sent[9]))\n",
    "    \n",
    "    df = pd.DataFrame(items_output)\n",
    "    df.to_csv(name +\"_items.csv\", index=False, header=False)\n",
    "    \n",
    "    with open(name+\"_exposure_tests.txt\", \"w\") as outf:\n",
    "        outf.writelines(\"\\n\".join([x[5] for x in test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLURAL AGREEMENT LEARNING\n",
    "    \n",
    "def n_distractor(is_plural):\n",
    "    if is_plural:\n",
    "        return random.choice(ITEM_BLOCKS_SING)\n",
    "    else:\n",
    "        return random.choice(ITEM_BLOCKS_PL)\n",
    "\n",
    "def generate_plural_tests(noun_bundle, is_plural):\n",
    "    \n",
    "    test_items = []\n",
    "    \n",
    "    for i in range(N_ITEMS_PER_TEST):\n",
    "        \n",
    "        det1 = random.choice(DET_BLOCKS)\n",
    "        det2 = random.choice(DET_BLOCKS)\n",
    "        verb1 = random.choice(V_TR_BLOCKS)\n",
    "        prep1 = random.choice(PREPS)\n",
    "        adj1 = random.choice(ADJ_BLOCKS)\n",
    "        n_distr = n_distractor(is_plural)\n",
    "        intensifier1 = random.choice(INTENSE_BLOCKS)\n",
    "        adj2 = random.choice(list(set(ADJ_BLOCKS)-set([adj1])))\n",
    "        sing_be = random.choice(SING_BE)\n",
    "        pl_be = random.choice(PL_BE)\n",
    "\n",
    "        \n",
    "        # BASE SIMPLE: The NOUN is/are\n",
    "        base_simple_sing = \" \".join([det1.capitalize(), noun_bundle[0], \"is\", \".\" ])\n",
    "        base_simple_pl = \" \".join([det1.capitalize(), noun_bundle[0], \"are\", \".\"])\n",
    "        \n",
    "        # Word, POS, Frequency, Test Sentence, Grammaticality, Target Index, Item Number, Test Number\n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], base_simple_sing, \"sing\", 3, i, \"base_simple\") )\n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], base_simple_pl, \"pl\", 3, i, \"base_simple\") )\n",
    "    \n",
    "        # BASE RC: The noun who the president likes is/are\n",
    "        base_rc_sing = \" \".join([det1.capitalize(), noun_bundle[0], \"who\", det2, n_distr, verb1, \"is\", \".\"])\n",
    "        base_rc_pl = \" \".join([det1.capitalize(), noun_bundle[0], \"who\", det2, n_distr, verb1,  \"are\", \".\"])\n",
    "        \n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], base_rc_sing, \"sing\", 7, i, \"base_rc\") )\n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], base_rc_pl, \"pl\", 7, i, \"base_rc\") )\n",
    "    \n",
    "        # BASE PREP: The noun near the president is/are\n",
    "        base_pp_sing = \" \".join([det1.capitalize(), noun_bundle[0], prep1, det2, adj1, n_distr, verb1, \"is\", \".\"])\n",
    "        base_pp_pl = \" \".join([det1.capitalize(), noun_bundle[0], prep1, det2, adj1, n_distr, verb1,  \"are\", \".\"])\n",
    "        \n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], base_pp_sing, \"sing\", 8, i, \"base_pp\") )\n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], base_pp_pl, \"pl\", 8, i, \"base_pp\") )\n",
    "    \n",
    "        # TRANSF SIMPLE: Are/Is the NOUN\n",
    "        transf_simple_sing = \" \".join([sing_be.capitalize(), det1, noun_bundle[0], \".\"])\n",
    "        transf_simple_pl = \" \".join([pl_be.capitalize(), det1, noun_bundle[0], \".\"])\n",
    "        \n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], transf_simple_sing, \"sing\", 3, i, \"transf_simple\") )\n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], transf_simple_pl, \"pl\", 3, i, \"transf_simple\") )\n",
    "        \n",
    "        #TRANSF MOD: Are/Is the very important and big NOUN\n",
    "        transf_mod_sing = \" \".join([sing_be.capitalize(), det1, intensifier1, adj1, \"and\", adj2, noun_bundle[0], \".\"])\n",
    "        transf_mod_pl = \" \".join([pl_be.capitalize(), det1, intensifier1, adj1, \"and\", adj2, noun_bundle[0], \".\"])\n",
    "        \n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], transf_mod_sing, \"sing\", 7, i, \"transf_mod\") )\n",
    "        test_items.append( (noun_bundle[0], noun_bundle[2], noun_bundle[3], transf_mod_pl, \"pl\", 7, i, \"transf_mod\") )\n",
    "        \n",
    "    return test_items\n",
    "\n",
    "\n",
    "nouns = data[(data[\"XPOS\"] == \"NN\") | (data[\"XPOS\"] == \"NNS\")]\n",
    "nouns = nouns[nouns[\"WORD\"].str.islower()]\n",
    "nouns = nouns[nouns[\"WORD\"].isin(train_items)]\n",
    "n_counts = nouns.groupby([\"WORD\", \"LEMMA\", \"XPOS\"]).size().reset_index()\n",
    "\n",
    "n_sing = sample_words(n_counts[n_counts[\"XPOS\"]==\"NN\"])\n",
    "n_pl = sample_words(n_counts[n_counts[\"XPOS\"] == \"NNS\"])\n",
    "\n",
    "singulars = [generate_plural_tests(x, False) for x in n_sing]\n",
    "plurals = [generate_plural_tests(x, True) for x in n_pl]\n",
    "\n",
    "export_plural_test(plurals+singulars, \"number-downsample\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENT STRUCTURE LEARNING\n",
    "\n",
    "def generate_argstruct_tests(verb_bundle, is_pres):\n",
    "    \n",
    "    test_items = []\n",
    "    \n",
    "    \"\"\"\n",
    "    Argument Structure Tests\n",
    "        - The lion devoured the gazelle yesterday . [OK for Trans, Not OK for Intrans]\n",
    "        - The lion devoured yesterday . [Not OK for Trans, OK for intrans]\n",
    "        \n",
    "        - The gazelle was devoured yesterday . [OK for trans, not OK for intrans]\n",
    "        - The gazelle devoured yesterday . [Not OK for trans, OK for intrans]\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(N_ITEMS_PER_TEST):\n",
    "        \n",
    "        det1 = random.choice(DET_BLOCKS)\n",
    "        noun1 = random.choice(NOUN_BLOCKS)\n",
    "        det2 = random.choice(DET_BLOCKS)\n",
    "        noun2 = random.choice(NOUN_BLOCKS)\n",
    "        adv3 = random.choice(PAST_ADVS)\n",
    "        aux = random.choice(AUX)\n",
    "        \n",
    "        adv1 = random.choice(PRES_ADVS)\n",
    "        adv2 = random.choice(list(set(PRES_ADVS)-set([adv1])))\n",
    "        adv4 = random.choice(list(set(PRES_ADVS)-set([adv1, adv2])))\n",
    "\n",
    "        \n",
    "        if is_pres:\n",
    "            \n",
    "            # BASE TEST\n",
    "            test_base_obj = \" \".join([det1.capitalize(), noun1, aux, verb_bundle[0], det2, noun2, \"today\", \".\"])\n",
    "            test_base_nobj = \" \".join([det1.capitalize(), noun1, aux, verb_bundle[0],  \"today\", \".\"])\n",
    "\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_base_obj, \"obj\", 8, i, \"base-pres\") )\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_base_nobj, \"nobj\", 6, i, \"base-pres\") )\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            # BASE TEST\n",
    "            test_base_obj = \" \".join([det1.capitalize(), noun1, verb_bundle[0], det2, noun2, adv3, \".\"])\n",
    "            test_base_nobj = \" \".join([det1.capitalize(), noun1, verb_bundle[0],  adv3, \".\"])\n",
    "\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_base_obj, \"obj\", 7, i, \"base-nomod\") )\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_base_nobj, \"nobj\", 5, i, \"base-nomod\") )\n",
    "\n",
    "            # TRANSF TEST\n",
    "            test_transf_obj = \" \".join([det1.capitalize(), noun1, \"was\", verb_bundle[0], adv3, \".\"])\n",
    "            test_transf_nobj = \" \".join([det1.capitalize(), noun1, verb_bundle[0], adv3, \".\"])\n",
    "\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_transf_obj, \"obj\", 6, i, \"transf-nomod\") )\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_transf_nobj, \"nobj\", 5, i, \"transf-nomod\") )\n",
    "\n",
    "            # TRANSF TEST Short Modification\n",
    "            test_transf_obj = \" \".join([det1.capitalize(), noun1, \"was\", adv1, \"and\", adv2, verb_bundle[0], adv3, \".\"])\n",
    "            test_transf_nobj = \" \".join([det1.capitalize(), noun1, adv1, \"and\", adv2, verb_bundle[0], adv3, \".\"])\n",
    "\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_transf_obj, \"obj\", 9, i, \"transf-mod\") )\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_transf_nobj, \"nobj\", 8, i, \"transf-mod\") )\n",
    "\n",
    "            # TRANSF TEST Long Modification\n",
    "            test_transf_obj = \" \".join([det1.capitalize(), noun1, \"was\", adv1, \",\", adv2, \",\", \"and\", adv4, verb_bundle[0], adv3, \".\"])\n",
    "            test_transf_nobj = \" \".join([det1.capitalize(), noun1, adv1, \",\", adv2, \",\", \"and\", adv4, verb_bundle[0], adv3, \".\"])\n",
    "\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_transf_obj, \"obj\", 12, i, \"transf-longmod\") )\n",
    "            test_items.append( (verb_bundle[0], verb_bundle[2], verb_bundle[3], verb_bundle[5], verb_bundle[6], test_transf_nobj, \"nobj\", 11, i, \"transf-longmod\") )\n",
    "\n",
    "        \n",
    "    return test_items\n",
    "\n",
    "\n",
    "verbs = data[(data[\"XPOS\"] == \"VBD\") | (data[\"XPOS\"] == \"VB\")]\n",
    "verbs = verbs[verbs[\"WORD\"].str.islower()]\n",
    "verbs = verbs[verbs[\"WORD\"].isin(train_items)]\n",
    "\n",
    "v_counts = verbs.groupby([\"WORD\", \"LEMMA\", \"XPOS\"]).size().reset_index()\n",
    "\n",
    "celex = pd.read_csv(\"~/Documents/resources/data/celex2/english/esl/esl.cd\", sep=\"\\\\\", header=None)\n",
    "# Trans_V = Can sometimes take a direct object\n",
    "# Intrans_V = Can sometimes be intransitive\n",
    "celex.columns = [\"IdNum\",\"word\", \"Cob\", \"ClassNum\", \"C_N\", \"Unc_N\", \"Sing_N\", \"Plu_N\", \"GrC_N\",\n",
    "                 \"GrUnc_N\", \"Attr_N\", \"PostPos_N\", \"Voc_N\",\"Proper_N\", \"Exp_N\",\"Trans_V\",\"TransComp_V\",\n",
    "                 \"Intrans_V\",\"Ditrans_V\",\"Link_V\",\"Phr_V\",\"Prep_V\",\"PhrPrep_V\",\"Exp_V\",\"Ord_A\",\"Attr_A\",\n",
    "                 \"Pred_A\",\"PostPos_A\",\"Exp_A\",\"Ord_ADV\",\"Pred_ADV\",\"PostPos_ADV\",\"Comb_ADV\",\"Exp_ADV\",\n",
    "                 \"Card_NUM\",\"Ord_NUM\",\"Exp_NUM\",\"Pers_PRON\",\"Dem_PRON\",\"Poss_PRON\",\"Refl_PRON\",\"Wh_PRON\",\n",
    "                 \"Det_PRON\",\"Pron_PRON\",\"Exp_PRON\",\"Cor_C\",\"Sub_C\"]\n",
    "\n",
    "argstruct = celex[[\"word\", \"Trans_V\", \"Intrans_V\",]]\n",
    "v_counts = v_counts.merge(argstruct, left_on=\"LEMMA\", right_on='word')\n",
    "\n",
    "\n",
    "vbd_trans = sample_words(v_counts[(v_counts[\"Trans_V\"] == \"Y\") & (v_counts[\"Intrans_V\"] == \"N\") & (v_counts[\"XPOS\"] == \"VBD\")])\n",
    "vbd_intrans = sample_words(v_counts[(v_counts[\"Trans_V\"] == \"N\") & (v_counts[\"Intrans_V\"] == \"Y\") & (v_counts[\"XPOS\"] == \"VBD\")])\n",
    "vbd_ambi = sample_words(v_counts[(v_counts[\"Trans_V\"] == \"Y\") & (v_counts[\"Intrans_V\"] == \"Y\") & (v_counts[\"XPOS\"] == \"VBD\")])\n",
    "\n",
    "trans = [generate_argstruct_tests(x, False) for x in vbd_trans]\n",
    "intrans = [generate_argstruct_tests(x, False) for x in vbd_intrans]\n",
    "ambi = [generate_argstruct_tests(x, False) for x in vbd_ambi]\n",
    "\n",
    "vb_trans = sample_words(v_counts[(v_counts[\"Trans_V\"] == \"Y\") & (v_counts[\"Intrans_V\"] == \"N\") & (v_counts[\"XPOS\"] == \"VB\")])\n",
    "vb_intrans = sample_words(v_counts[(v_counts[\"Trans_V\"] == \"N\") & (v_counts[\"Intrans_V\"] == \"Y\") & (v_counts[\"XPOS\"] == \"VB\")])\n",
    "vb_ambi = sample_words(v_counts[(v_counts[\"Trans_V\"] == \"Y\") & (v_counts[\"Intrans_V\"] == \"Y\") & (v_counts[\"XPOS\"] == \"VB\")])\n",
    "\n",
    "trans_pres = [generate_argstruct_tests(x, True) for x in vb_trans]\n",
    "intrans_pres = [generate_argstruct_tests(x, True) for x in vb_intrans]\n",
    "ambi_pres = [generate_argstruct_tests(x, True) for x in vb_ambi]\n",
    "\n",
    "export_argstruct_test(trans+intrans+ambi+trans_pres+intrans_pres+ambi_pres, \"argstruct-downsample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
